{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d14dae7737b34168a1840ff4e23b98e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d3676acbc6f413dbb8ba37c1646ec65",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_25a6c8bc3c284b67972e7786ea347427",
              "IPY_MODEL_a0c4f9498f2f482b94683c953a1f8771"
            ]
          }
        },
        "4d3676acbc6f413dbb8ba37c1646ec65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25a6c8bc3c284b67972e7786ea347427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_85af185adb28417da414d275427a0c8e",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 56,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16815a9aaf714f7f9fd96c466a672586"
          }
        },
        "a0c4f9498f2f482b94683c953a1f8771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_549abc4fd1bc432ab979629a3018e0e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/56 [56:52&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_087e61c00b284fc2a665c9f5aa6052d1"
          }
        },
        "85af185adb28417da414d275427a0c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16815a9aaf714f7f9fd96c466a672586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "549abc4fd1bc432ab979629a3018e0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "087e61c00b284fc2a665c9f5aa6052d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4gZxaW8xaJ1j",
        "outputId": "29a80778-ec6a-4af9-ec3e-62300244f2f6"
      },
      "source": [
        "import pandas as pd\r\n",
        "import gensim\r\n",
        "from transformers import AutoTokenizer\r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "\r\n",
        "\r\n",
        "gensim.__version__"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.8.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DRoZw17aM5p"
      },
      "source": [
        "#!pip install --upgrade gensim\r\n",
        "#!pip install --upgrade pandas\r\n",
        "#!pip install tensorflow_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Bx989q0aY4t"
      },
      "source": [
        "documents = [\r\n",
        "    \"Human machine interface for lab abc computer applications\",\r\n",
        "    \"A survey of user opinion of computer system response time\",\r\n",
        "    \"The EPS user interface management system\",\r\n",
        "    \"System and human system engineering testing of EPS\",\r\n",
        "    \"Relation of user perceived response time to error measurement\",\r\n",
        "    \"The generation of random binary unordered trees\",\r\n",
        "    \"The intersection graph of paths in trees\",\r\n",
        "    \"Graph minors IV Widths of trees and well quasi ordering\",\r\n",
        "    \"Graph minors A survey\",\r\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbUNxyL_a8k1",
        "outputId": "5e637dd8-22a5-455f-b835-6a5b136a72b7"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords,preprocess_documents\r\n",
        "from pprint import pprint\r\n",
        "pprint([remove_stopwords(doc) for doc in documents])\r\n",
        "documents2 = preprocess_documents(documents)\r\n",
        "documents2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Human machine interface lab abc applications',\n",
            " 'A survey user opinion response time',\n",
            " 'The EPS user interface management',\n",
            " 'System human engineering testing EPS',\n",
            " 'Relation user perceived response time error measurement',\n",
            " 'The generation random binary unordered trees',\n",
            " 'The intersection graph paths trees',\n",
            " 'Graph minors IV Widths trees quasi ordering',\n",
            " 'Graph minors A survey']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'machin', 'interfac', 'lab', 'abc', 'applic'],\n",
              " ['survei', 'user', 'opinion', 'respons', 'time'],\n",
              " ['ep', 'user', 'interfac', 'manag'],\n",
              " ['human', 'engin', 'test', 'ep'],\n",
              " ['relat', 'user', 'perceiv', 'respons', 'time', 'error', 'measur'],\n",
              " ['gener', 'random', 'binari', 'unord', 'tree'],\n",
              " ['intersect', 'graph', 'path', 'tree'],\n",
              " ['graph', 'minor', 'width', 'tree', 'quasi', 'order'],\n",
              " ['graph', 'minor', 'survei']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFzJyz31cfZl"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jukgIfdseEsW"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8HKN5Gdcf0q"
      },
      "source": [
        "# Word2Vec\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMZBxLZ4ckVS",
        "outputId": "f2bd0dc8-ad12-4e65-8553-67f4fb7322ee"
      },
      "source": [
        "vecsize=128\r\n",
        "documents\r\n",
        "from gensim.test.utils import common_texts\r\n",
        "\r\n",
        "\r\n",
        "documents\r\n",
        "common_texts\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer'],\n",
              " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNFYzF39cewi"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "\r\n",
        "model = Word2Vec(sentences=common_texts, size=vecsize, window=5, min_count=1, workers=4)\r\n",
        "model = Word2Vec(sentences=documents, size=vecsize, window=5, min_count=1, workers=4)\r\n",
        "model.save(\"word2vec.model\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcEWHamvhR0R"
      },
      "source": [
        "# Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEbFo-12akrS"
      },
      "source": [
        "from gensim.test.utils import common_texts\r\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\r\n",
        "#TaggedDocument?\r\n",
        "\r\n",
        "#This I don't understand:\r\n",
        "#\"Tags may be one or more unicode string tokens, but typical practice (which will also be the most memory-efficient) is for the tags list to include a unique integer id as the only tag.\"\r\n",
        "#What kind of meaning do these tags then contain?!"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SiQ3ZejdQ3_"
      },
      "source": [
        "documents3 = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\r\n",
        "model = Doc2Vec(documents3, vector_size=128, window=2, min_count=1, workers=4)\r\n",
        "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysqHDtmTb8L8",
        "outputId": "c23e3f76-4b97-40ed-cf15-253b86344a03"
      },
      "source": [
        "%%timeit\r\n",
        "vector = model.infer_vector([\"system\", \"response\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 7.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 382 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74EwBNWWgQUe",
        "outputId": "3fe51ec6-7962-4a0b-df72-02b90a4e8183"
      },
      "source": [
        "vector = model.infer_vector([\"system\", \"response\"])\r\n",
        "vector.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srLZBqXRhtb3"
      },
      "source": [
        "## Bert\r\n",
        "Choose a model from here: [https://huggingface.co/models](https://huggingface.co/models).\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So_LcYpchxev"
      },
      "source": [
        "\"\"\"\r\n",
        "!pip install -U Huggingface\r\n",
        "!pip install -U sentence-transformers\r\n",
        "\"\"\";"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwTaS276gUMt"
      },
      "source": [
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
        "sbert_model?"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "4lfBsFjQif4z",
        "outputId": "1904b56c-59f3-4ecc-cb41-1096d90381df"
      },
      "source": [
        "sentences = [['This framework generates embeddings for each input sentence'],\r\n",
        "    ['Sentences are passed as a list of string.'], \r\n",
        "    ['The quick brown fox jumps over the lazy dog.','Here I am wondering how multi-sentence documents are used.','This seems not the right way. Probably a list of lists of words is necessary?' ],\r\n",
        "    ['Well, with another iterator it works.' ]]\r\n",
        "documents_df = pd.DataFrame({'documents_cleaned': sentences})\r\n",
        "documents_df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>documents_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[This framework generates embeddings for each ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Sentences are passed as a list of string.]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The quick brown fox jumps over the lazy dog.,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Well, with another iterator it works.]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   documents_cleaned\n",
              "0  [This framework generates embeddings for each ...\n",
              "1        [Sentences are passed as a list of string.]\n",
              "2  [The quick brown fox jumps over the lazy dog.,...\n",
              "3            [Well, with another iterator it works.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGwPVFNujLbV",
        "outputId": "fc1d2baa-89f6-4b75-ccc4-f98baf11bd08"
      },
      "source": [
        "sentence_embeddings = [sbert_model.encode(doc) for doc in documents_df.documents_cleaned]\r\n",
        "\r\n",
        "print(\"Sentence embeddings:\")\r\n",
        "print(sentence_embeddings[0].shape)\r\n",
        "print(sentence_embeddings[-2].shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence embeddings:\n",
            "(1, 768)\n",
            "(3, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "piU1bbh0lXPx",
        "outputId": "ac19a38f-dffa-4b46-a73f-049fce098abd"
      },
      "source": [
        "from transformers import AutoTokenizer\r\n",
        "\r\n",
        "#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\r\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\r\n",
        "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\r\n",
        "print(encoded_input)\r\n",
        "decoded_input = tokenizer.decode(encoded_input[\"input_ids\"])\r\n",
        "decoded_input"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 7592, 1010, 1045, 1005, 1049, 1037, 2309, 6251, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[CLS] hello, i'm a single sentence! [SEP]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrPUnGMYjtuN"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc04Z82ojZqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "c09e387d-2493-45df-a2a0-bff060f0c73a"
      },
      "source": [
        "documents_df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>documents_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[This framework generates embeddings for each ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Sentences are passed as a list of string.]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The quick brown fox jumps over the lazy dog.,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Well, with another iterator it works.]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   documents_cleaned\n",
              "0  [This framework generates embeddings for each ...\n",
              "1        [Sentences are passed as a list of string.]\n",
              "2  [The quick brown fox jumps over the lazy dog.,...\n",
              "3            [Well, with another iterator it works.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_CQqqaXhwT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a33d11a-ed08-48dd-edda-5a62627ee9f2"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\r\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
        "sentences = ['This framework generates embeddings for each input sentence',\r\n",
        "    'Sentences are passed as a list of string.', \r\n",
        "    'The quick brown fox jumps over the lazy dog.']\r\n",
        "sentence_embeddings = model.encode(sentences)\r\n",
        "\r\n",
        "print(\"Sentence embeddings:\")\r\n",
        "print(sentence_embeddings)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence embeddings:\n",
            "[[-0.10409425  0.5274767   1.1797731  ... -0.43389115 -0.6945233\n",
            "   0.5386927 ]\n",
            " [-0.13118456 -0.17390285  1.1052188  ...  0.02624461 -0.00269846\n",
            "   0.916111  ]\n",
            " [-0.7489929   0.71891737 -1.039457   ...  0.15582633  1.0202514\n",
            "   0.09790451]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M00wzG_YsTPa"
      },
      "source": [
        "# Using Mirco's Benchmarking Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQf7-L1GrC0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939aced5-d2fc-456b-d922-1bca1dd9bf5b"
      },
      "source": [
        "!pwd -P\r\n",
        "%cd /home"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/home\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKuJYVYUsWXk",
        "outputId": "ce9c9e5c-8ab4-4ed5-c6a2-e2a6ef1cd4b4"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkhzMAAQsv71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ccbf66-1bce-4cc0-ecb0-bc25c991704e"
      },
      "source": [
        "!ls \"/content/gdrive/MyDrive/Colab Notebooks/soundofai\""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embeddings.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jn681khykXV",
        "outputId": "11904cc8-244e-47ad-8a33-e50fdcccb587"
      },
      "source": [
        "!git clone --branch toedtli_mytestbranch https://toedtli:m2y7s4upers73ecret%21password@github.com/toedtli/soundofai.git"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'soundofai' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3WnROXPtJR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f61186-06ee-49e7-a9a4-d9e8956853ad"
      },
      "source": [
        "!rm -rf soundofai\r\n",
        "import os\r\n",
        "from getpass import getpass\r\n",
        "import urllib\r\n",
        "\r\n",
        "user = input('User name: ')\r\n",
        "password = getpass('Password: ')\r\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\r\n",
        "#repo_name = input('Repo name: ')\r\n",
        "repo_name = 'soundofai'\r\n",
        "branch = input('branch name: ')\r\n",
        "cmd_string = f'git clone --branch {branch} https://{user}:{password}@github.com/{user}/{repo_name}.git'\r\n",
        "\r\n",
        "os.system(cmd_string)\r\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: toedtli\n",
            "Password: ··········\n",
            "branch name: toedtli_mytestbranch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcIvF5MlwK8Z",
        "outputId": "f5bbf780-ca20-45c9-8d9c-1dc1c83bb8db"
      },
      "source": [
        "!ls soundofai"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md\t\t\t\ttext_embedders_benchmark_preview.py\n",
            "text_embedders_benchmark_preview.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsxwZc9rwY1y",
        "outputId": "55649214-b4e4-45b7-bd44-07aa38e7e128"
      },
      "source": [
        "%cd /home/soundofai\r\n",
        "#!pip install tensorflow_text\r\n",
        "from text_embedders_benchmark_preview import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/soundofai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWDympjX2XxG"
      },
      "source": [
        "class GensimPredictionModelWithPreprocessor(PredictionModel):\r\n",
        "  family='Gensim'\r\n",
        "  def build(self):\r\n",
        "    #text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\r\n",
        "    #preprocessor = hub.KerasLayer(self.preprocessor_url)\r\n",
        "    self.preprocessor = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\r\n",
        "    encoder_inputs = self.preprocessor\r\n",
        "    self.model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
        "\r\n",
        "  def predict(self, sentences):\r\n",
        "    output_tensor = self.model.encode(sentences)\r\n",
        "  \r\n",
        "    return output_tensor\r\n",
        "\r\n",
        "  def additional_infos(self):\r\n",
        "        return {\r\n",
        "        \"general\":\"This is not a tf-hub model, but a gensim model. maybe need a type key here\",\r\n",
        "        \"tf_hub_url\":None,\r\n",
        "        \"family\":self.family,\r\n",
        "        \"word_level_output_available\":True\r\n",
        "    }\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RdOLiWiB3Tb"
      },
      "source": [
        "def benchmark_prediction_model(model_name, sentences, results=None):\r\n",
        "  model=eval(f\"{model_name}()\")\r\n",
        "\r\n",
        "  if results is None:\r\n",
        "    results={}\r\n",
        "  \r\n",
        "  results[\"model_name\"]=model_name\r\n",
        "  \r\n",
        "  print(f\"{model_name} - building...\")\r\n",
        "  now=time.time()\r\n",
        "  model.build()\r\n",
        "  results[\"build_seconds\"]=time.time()-now\r\n",
        "  \r\n",
        "  print(f\"{model_name} - first prediction...\")\r\n",
        "  now=time.time()\r\n",
        "  prediction = model.predict(sentences)\r\n",
        "  results[\"first_prediction_seconds\"]=time.time()-now\r\n",
        "  \r\n",
        "  print(f\"{model_name} - second prediction...\")\r\n",
        "  now=time.time()\r\n",
        "  prediction = model.predict(sentences)\r\n",
        "  results[\"second_prediction_seconds\"]=time.time()-now\r\n",
        "\r\n",
        "  results[\"embedding_size\"]=prediction.shape[1]\r\n",
        "  results[\"additional_infos\"]=json.dumps(model.additional_infos())\r\n",
        "\r\n",
        "  return results"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJFWV9JhDcbM",
        "outputId": "67bce236-2cc4-43e3-fc22-882f9f41c477"
      },
      "source": [
        "model = GensimPredictionModelWithPreprocessor()\r\n",
        "model.build()\r\n",
        "model.predict(sentences)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.46516186, -0.579506  ,  0.23042098, ...,  0.6943612 ,\n",
              "        -0.13120279, -0.73131865],\n",
              "       [-0.7033002 , -0.5974654 ,  0.5133683 , ..., -0.05653809,\n",
              "        -0.18607607,  0.35441774],\n",
              "       [-0.04879691, -0.01012288,  0.47366896, ...,  0.72819793,\n",
              "        -0.07263067,  0.06405059],\n",
              "       ...,\n",
              "       [-0.31490332,  0.7199607 , -0.13828947, ...,  0.05519799,\n",
              "         0.33923128, -0.1394053 ],\n",
              "       [ 0.01398076,  0.33343962,  0.91195005, ...,  1.1070508 ,\n",
              "        -0.23172055,  0.07168981],\n",
              "       [ 0.10315251, -0.5857194 ,  1.0750997 , ..., -0.32626587,\n",
              "        -0.44016764, -0.8372998 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYGZyFM0CWxI",
        "outputId": "ca17a01c-d35d-4c9e-b6a9-c6e0cd509483"
      },
      "source": [
        "benchmark_prediction_model('GensimPredictionModelWithPreprocessor', sentences)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GensimPredictionModelWithPreprocessor - building...\n",
            "GensimPredictionModelWithPreprocessor - first prediction...\n",
            "GensimPredictionModelWithPreprocessor - second prediction...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'additional_infos': '{\"general\": \"This is not a tf-hub model, but a gensim model. maybe need a type key here\", \"tf_hub_url\": null, \"family\": \"Gensim\", \"word_level_output_available\": true}',\n",
              " 'build_seconds': 4.127309560775757,\n",
              " 'embedding_size': 768,\n",
              " 'first_prediction_seconds': 1.590348720550537,\n",
              " 'model_name': 'GensimPredictionModelWithPreprocessor',\n",
              " 'second_prediction_seconds': 1.6815154552459717}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxpHphIVCTQW",
        "outputId": "3a758073-785a-43d5-bfd0-415ad2eabe3f"
      },
      "source": [
        "from text_embedders_benchmark_preview import *\r\n",
        "tools=BenchmarkingTools()\r\n",
        "p = 'GensimPredictionModelWithPreprocessor'\r\n",
        "safe_benchmark_prediction_model(GensimPredictionModelWithPreprocessor,sentences)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzVsiIsc94Q6",
        "outputId": "4cc7b4d5-909e-462e-c722-a30195c4e762"
      },
      "source": [
        "sentences"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Give me a bright guitar',\n",
              " \"I'd like a sharp cello\",\n",
              " 'give me a dry acoustic guitar',\n",
              " 'give me a metallic harp',\n",
              " 'give me a dirty organ',\n",
              " 'give me a hollow piano',\n",
              " 'give me a sharp trumpet',\n",
              " 'give me a cold triangle',\n",
              " 'give me dark drums',\n",
              " 'give me a soft french horn',\n",
              " 'give me a dull clarinet',\n",
              " 'give me a smooth operator',\n",
              " 'Give me a simple square bass',\n",
              " 'Give me an orchestral string',\n",
              " 'Give me an analog pad',\n",
              " 'Give me a simple sine bass',\n",
              " 'Give me a chord preset',\n",
              " 'Get me a 909 closed hi-hat',\n",
              " 'Get me an 808 open hi-hat',\n",
              " 'Give me a round bass',\n",
              " 'Give me a sharp synth',\n",
              " 'Give me a warm pad',\n",
              " 'Give me a wide stereo pad',\n",
              " 'Give me a mono, warm, round synth bass',\n",
              " 'Make me a soft flute that sounds like a chirping bird ',\n",
              " 'Give me a dark brassy sound',\n",
              " 'Can you give me a wailing guitar?',\n",
              " 'Get me a scratchy violin',\n",
              " 'Give me a Star Wars laser beam sound',\n",
              " 'Can you combine a low piano sound with a roaring lion?',\n",
              " 'Get me something like a compact bleep',\n",
              " 'Give me a funky guitar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "PWDEbC1Y-8UB",
        "outputId": "70cce13f-b6fc-4557-fed7-f3e0158a3833"
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-0445ab8a71a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark_prediction_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GensimPredictionModelWithPreprocessor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-97-d5ba3ceac388>\u001b[0m in \u001b[0;36mbenchmark_prediction_model\u001b[0;34m(model_name, sentences, results)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbenchmark_prediction_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class GensimPredictionModelWithPreprocessor with abstract methods predict"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "MCXYgAMaB8WE",
        "outputId": "f85dfb7d-3ed6-451f-9206-8b081ebf721d"
      },
      "source": [
        "GensimPredictionModelWithPreprocessor()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-f9a37e060650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGensimPredictionModelWithPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class GensimPredictionModelWithPreprocessor with abstract methods predict"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d14dae7737b34168a1840ff4e23b98e7",
            "4d3676acbc6f413dbb8ba37c1646ec65",
            "25a6c8bc3c284b67972e7786ea347427",
            "a0c4f9498f2f482b94683c953a1f8771",
            "85af185adb28417da414d275427a0c8e",
            "16815a9aaf714f7f9fd96c466a672586",
            "549abc4fd1bc432ab979629a3018e0e0",
            "087e61c00b284fc2a665c9f5aa6052d1"
          ]
        },
        "id": "yt-lJgEE1Owl",
        "outputId": "1327c89e-775d-4ed5-8fb0-58815102d387"
      },
      "source": [
        "prediction_models\r\n",
        "tools=BenchmarkingTools()\r\n",
        "results = []\r\n",
        "for p in tqdm(prediction_models):\r\n",
        "  r=tools.benchmark_and_cleanup(p, sentences)\r\n",
        "  #r=benchmark_prediction_model(p, sentences)\r\n",
        "  results.append(r)\r\n",
        "\r\n",
        "df=pd.DataFrame(results)\r\n",
        "df.to_csv(\"results.csv\", index=False)\r\n",
        "df"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d14dae7737b34168a1840ff4e23b98e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=56.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "talkheads_ggelu_bert_en_large - building...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n",
            "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3, Total size: 1.96MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1'.\n",
            "Process Process-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 504, in safe_benchmark_prediction_model\n",
            "    benchmark_prediction_model(model_name, sentences, results)\n",
            "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 480, in benchmark_prediction_model\n",
            "    model.build()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-9a4156ac7630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark_and_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m#r=benchmark_prediction_model(p, sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/soundofai/text_embedders_benchmark_preview.py\u001b[0m in \u001b[0;36mbenchmark_and_cleanup\u001b[0;34m(self, model_name, sentences)\u001b[0m\n\u001b[1;32m    526\u001b[0m         target=safe_benchmark_prediction_model, args=(model_name, sentences, return_dict))\n\u001b[1;32m    527\u001b[0m     \u001b[0mprocess_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mprocess_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 76, in build\n",
            "    encoder = hub.KerasLayer(self.tf_hub_url, trainable=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 152, in __init__\n",
            "    self._func = load_module(handle, tags, self._load_options)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 427, in load_module\n",
            "    return module_v2.load(handle, tags=tags, options=set_load_options)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 92, in load\n",
            "    module_path = resolve(handle)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 47, in resolve\n",
            "    return registry.resolver(handle)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/registry.py\", line 51, in __call__\n",
            "    return impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\", line 68, in __call__\n",
            "    self._lock_file_timeout_sec())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\", line 409, in atomic_download\n",
            "    download_fn(handle, tmp_dir)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\", line 65, in download\n",
            "    response, tmp_dir)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\", line 190, in download_and_uncompress\n",
            "    fileobj, dst_path, log_function=self._log_progress)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/file_utils.py\", line 52, in extract_tarfile_to_destination\n",
            "    extract_file(tgz, tarinfo, abs_target_path, log_function=log_function)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/file_utils.py\", line 35, in extract_file\n",
            "    buf = src.read(buffer_size)\n",
            "  File \"/usr/lib/python3.6/tarfile.py\", line 708, in readinto\n",
            "    buf = self.read(len(b))\n",
            "  File \"/usr/lib/python3.6/tarfile.py\", line 697, in read\n",
            "    b = self.fileobj.read(length)\n",
            "  File \"/usr/lib/python3.6/tarfile.py\", line 539, in read\n",
            "    buf = self._read(size)\n",
            "  File \"/usr/lib/python3.6/tarfile.py\", line 556, in _read\n",
            "    buf = self.cmp.decompress(buf)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 507, in safe_benchmark_prediction_model\n",
            "    results[\"success\"]=False\n",
            "  File \"<string>\", line 2, in __setitem__\n",
            "  File \"/usr/lib/python3.6/multiprocessing/managers.py\", line 757, in _callmethod\n",
            "    kind, result = conn.recv()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
            "    buf = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3wxorzL1vBE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}